{"cells":[{"metadata":{"_uuid":"2d302dbd1d82609bffed112b0cef299282fdfa9b"},"cell_type":"markdown","source":"**Organic chemistry reaction prediction using neural machine translation (NMT).**"},{"metadata":{"_uuid":"ed46ea034eb23195d03d4305aca57bcc126fb897"},"cell_type":"markdown","source":"This seq2seq with attention uses Asynchronous Bidirectional Decoding with beam search for predicting reactions in SMILES format. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional, Concatenate, Dot, Input, Permute, Reshape\nfrom keras.layers import RepeatVector, Dense, Activation, GRU, Lambda, Add\nfrom keras import optimizers, regularizers, initializers\nfrom keras.engine.topology import Layer\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nimport keras.backend as K\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\nimport random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(40)\n\ndef preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n    \n    X, Y = zip(*dataset)\n    \n    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n    \n    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n\n    return X, np.array(Y), Xoh, Yoh\n        \n        \ndef string_to_int(string, length, vocab):\n    \"\"\"\n    Converts all strings in the vocabulary into a list of integers representing the positions of the\n    input string's characters in the \"vocab\"\n    \n    Arguments:\n    string -- input string\n    length -- the number of time steps you'd like, determines if the output will be padded or cut\n    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n    \n    Returns:\n    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n    \"\"\"\n\n    u = vocab[\"<unk>\"]   \n    if len(string) > length:\n        string = string[:length]\n        \n    rep = list(map(lambda x: vocab.get(x, u), string))\n    \n    if len(string) < length:\n        rep += [vocab['<pad>']] * (length - len(string))\n    \n    #print (rep)\n    return rep\n\n\ndef int_to_string(ints, inv_vocab):\n    \"\"\"\n    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n    \n    Arguments:\n    ints -- list of integers representing indexes in the machine's vocabulary\n    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n    \n    Returns:\n    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n    \"\"\"\n    \n    l = [inv_vocab[i] for i in ints]\n    return l\n\n\ndef softmax(x, axis=-1):\n    \"\"\"Softmax activation function.\n    # Arguments\n        x : Tensor.\n        axis: Integer, axis along which the softmax normalization is applied.\n    # Returns\n        Tensor, output of softmax transformation.\n    # Raises\n        ValueError: In case `dim(x) == 1`.\n    \"\"\"\n    ndim = K.ndim(x)\n    if ndim == 2:\n        return K.softmax(x)\n    elif ndim > 2:\n        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n        s = K.sum(e, axis=axis, keepdims=True)\n        return e / s\n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Preprocessing the data:"},{"metadata":{"trusted":true,"_uuid":"54bc4f079456c07370dc9217c2a284257e3caf1a"},"cell_type":"code","source":"dataset = []\ninput_characters = set()\ntarget_characters = set()\n\ndata_path = '../input/ocrtrain.csv'\nlines = open(data_path).read().split('\\n')\n\nfor line in lines[0: len(lines) - 1]:\n    input_text, target_text = line.split(',')\n    input_text = input_text[1:-2]\n    input_text = input_text.split(' ')\n    target_text = target_text.split(' ')\n\n    if len(input_text)>=len(target_text) and len(input_text)<=15:\n        ds = (input_text,target_text)\n        dataset.append(ds)\n        for char in input_text:\n            if char not in input_characters:\n                input_characters.add(char)\n        for char in target_text:\n            if char not in target_characters:\n                target_characters.add(char)\n                \nz = np.array(dataset)\nTx = len(max(z[:,0], key=len))\nTy = len(max(z[:,1], key=len))\ntrain, test = train_test_split(dataset, test_size=0.05, random_state = 43)\ninput_characters = sorted(list(input_characters)) + ['<unk>', '<pad>']\ntarget_characters = sorted(list(target_characters)) + ['<unk>', '<pad>']\nreactants_vocab = {v:k for k,v in enumerate(input_characters)}\nproducts_vocab = {v:k for k,v in enumerate(target_characters)}\ninv_products_vocab = {v:k for k,v in products_vocab.items()}\n\ndataset = train\nX, Y, Xoh, Yoh = preprocess_data(dataset, reactants_vocab, products_vocab, Tx, Ty)\n\nprint(\"X.shape:\", X.shape)\nprint(\"Y.shape:\", Y.shape)\nprint(\"Xoh.shape:\", Xoh.shape)\nprint(\"Yoh.shape:\", Yoh.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47db415dcf6c0acf09b760f7d577cf1050740251"},"cell_type":"code","source":"index = 13\nprint(\"Reactants:\", dataset[index][0])\nprint(\"Product:\", dataset[index][1])\nprint()\nprint(\"Reactants after preprocessing (indices):\", X[index])\nprint(\"Product after preprocessing (indices):\", Y[index])\nprint()\nprint(\"Reactants after preprocessing (one-hot):\", Xoh[index])\nprint(\"Product after preprocessing (one-hot):\", Yoh[index])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0957ee64892a5128cf620d3076c20a0f7d8ca4df"},"cell_type":"markdown","source":"Attention Layer for the forward Decoder:"},{"metadata":{"trusted":true,"_uuid":"ba86cc46948856d35afffd89b3b4482667378454"},"cell_type":"code","source":"repeator = RepeatVector(1)\npermutor = Permute((2,1))\ndotor1 = Lambda(lambda x: K.batch_dot(x[0],x[1]))\nactivator = Activation('softmax')\ndotor2 = Lambda(lambda x: K.batch_dot(x[0],x[1]))\n\ndef one_step_attention(a, s_prev):\n    \"\"\"\n    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n    \"alphas\" and the hidden states \"a\" of the Bi-GRU.\n    \n    Arguments:\n    a -- hidden state output of the Bi-GRU, numpy-array of shape (m, Tx, n_a)\n    s_prev -- previous hidden state of the (post-attention) GRU, numpy-array of shape (m, n_s)\n    \n    Returns:\n    context -- context vector, input of the next (post-attetion) GRU cell\n    \"\"\"\n    \n    s_prev = repeator(s_prev)\n    a_trans = permutor(a)\n    alphas = dotor1([s_prev,a_trans])\n    alphas = activator(alphas)\n    c = dotor2([alphas,a])\n    \n    return c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d571aabe2cb3e99df32b3aece4eeb8eb6349845"},"cell_type":"markdown","source":"Attention Layer for the backward Decoder:"},{"metadata":{"trusted":true,"_uuid":"0dab2935335b17e80523774a9eb949d25acdd7be"},"cell_type":"code","source":"repeator_rev = RepeatVector(1)\npermutor_rev = Permute((2,1))\ndotor1_rev = Lambda(lambda x: K.batch_dot(x[0],x[1]))\nactivator_rev = Activation('softmax')\ndotor2_rev = Lambda(lambda x: K.batch_dot(x[0],x[1]))\n\ndef one_step_attention_rev(a, s_prev):\n    \"\"\"\n    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n    \"alphas\" and the hidden states \"a\" of the Bi-GRU.\n    \n    Arguments:\n    a -- hidden state output of the Bi-GRU, numpy-array of shape (m, Tx, n_a)\n    s_prev -- previous hidden state of the (post-attention) GRU, numpy-array of shape (m, n_a)\n    \n    Returns:\n    context -- context vector, input of the next (post-attetion) GRU cell\n    \"\"\"\n    \n    s_prev = repeator_rev(s_prev)\n    a_trans = permutor_rev(a)\n    alphas = dotor1_rev([s_prev,a_trans])\n    alphas = activator_rev(alphas)\n    c = dotor2_rev([alphas,a])\n    \n    return c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"867a18b3bbe9da0478f1dc9111ed93ab13e0da47"},"cell_type":"markdown","source":"Seq2Seq Model:\nSlightly based on the model discussed in \"Asynchronous Bidirectional Decoding for Neural Machine Translation\" (https://arxiv.org/abs/1801.05122)\n"},{"metadata":{"trusted":true,"_uuid":"51949118fafa72916293dde258361df4d1ad1925"},"cell_type":"code","source":"n_a = 256\nn_s = 256\nbatch_size = 128\npost_activation_GRU_cell = GRU(n_s,dropout=0.1,recurrent_dropout=0.1)\npost_activation_rev_GRU_cell = GRU(n_s,dropout=0.1,recurrent_dropout=0.1)\nadd_states1 = Add()\nadd_states2 = Add()\nreshaper1 = Reshape((n_s,))\nreshaper2 = Reshape((n_s,))\nconcatenator1 = Concatenate(axis=-1)\nconcatenator2 = Concatenate(axis=-1)\ndensor1 = Dense(n_s,activation='tanh')\ndensor2 = Dense(n_s,activation='tanh')\noutput_layer = Dense(len(products_vocab), activation='softmax',\n                     bias_initializer=initializers.Constant(value=0.025),\n                     activity_regularizer=regularizers.l2(0.05))\n\n\ndef model(Tx, Ty, n_a,n_s,reactants_vocab_size, products_vocab_size):\n    \"\"\"\n    Arguments:\n    Tx -- length of the input sequence\n    Ty -- length of the output sequence\n    n_a -- hidden state size of the Bi-LSTM\n    n_s -- hidden state size of the post-attention LSTM\n    human_vocab_size -- size of the python dictionary \"human_vocab\"\n    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    X = Input(shape=(Tx, reactants_vocab_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    s = s0\n    context_rev_seq = []\n    av2_seq = []\n    outputs = []\n    a = Bidirectional(GRU(n_a,return_sequences=True,\n                          recurrent_dropout=0.1),merge_mode='sum')(X)\n    a_rev = Lambda(lambda x: K.reverse(x,axes=1),output_shape=(Tx,n_a,))(a)\n    \n    for t in range(Ty):\n        context_rev = one_step_attention_rev(a_rev, s)\n        s = post_activation_rev_GRU_cell(context_rev, initial_state = s)\n        contextr = reshaper2(context_rev)\n        c2 = concatenator2([contextr,s])\n        av2 = densor2(c2)\n        av2_seq.append(av2)\n        context_rev_seq.append(context_rev)\n    \n    for t in range(Ty):\n        context = one_step_attention(a, s)\n        context_add = add_states1([context,context_rev_seq[Ty-1-t]])\n        s = post_activation_GRU_cell(context_add, initial_state = s)\n        context = reshaper1(context)\n        c1 = concatenator1([context,s])\n        av1 = densor1(c1)\n        av = add_states2([av1,av2_seq[Ty-1-t]])\n        out = output_layer(av)\n        outputs.append(out)\n        \n    model = Model(inputs = [X,s0], outputs = outputs)\n    return model\n\n\nmodel = model(Tx, Ty, n_a,n_s, len(reactants_vocab), len(products_vocab))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e316124dbe2f5fcbaef4bd607662e828e19b148"},"cell_type":"code","source":"m = len(dataset)\ns0 = np.zeros((m, n_s))\noutputs = list(Yoh.swapaxes(0,1))\n\nopt = optimizers.Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay=0.0005)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"419e334ff0d809ed70f42b225716aa93c9b7a2f3","scrolled":true},"cell_type":"code","source":"model.fit([Xoh,s0], outputs, batch_size = batch_size, epochs=130)\n#model.save('model256.h5')\nmodel.save_weights('weights256.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6f2617738ebabd95e3f40c7dd5207b3023a62c"},"cell_type":"code","source":"dataset = test\nX, Y, Xoh, Yoh = preprocess_data(dataset, reactants_vocab, products_vocab, Tx, Ty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e209e55bcf9cbb3aece6258619b2496507eee86"},"cell_type":"code","source":"def beam_search_decoder(data, k=3):\n    sequences = [[list(), 0.0]]\n    # walk over each step in sequence\n    for row in data:\n        all_candidates = list()\n        # expand each current candidate\n        for i in range(len(sequences)):\n            seq, score = sequences[i]\n            for j in range(len(row)):\n                candidate = [seq + [j], score-np.log(row[j])]\n                all_candidates.append(candidate)\n        # order all candidates by score\n        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n        # select k best\n        sequences = ordered[:k]\n    return sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b2f455782d43caa71f3211e3db991a97b9b244e"},"cell_type":"code","source":"output = []\npad = '<pad>'\nm = len(dataset)\ns0 = np.zeros((m, n_s))\nbleu_score = np.zeros((m,1))\n#prediction = model.predict([Xoh, s0, c0])\nprediction = model.predict([Xoh, s0])\ncount = 0\nbw = 5\nfor i in range(m):\n    p0 = np.array(prediction)[:,i,:]\n    p0 = beam_search_decoder(p0,bw)\n    for j in reversed(range(bw)):\n        p = p0[j][0]\n        p = int_to_string(p,inv_products_vocab)\n        o2 = []\n        for x in p:\n            if x != pad:\n                o2.append(x)\n        o1 = o2\n        o2 = ''.join(o2)\n        \n        if o2 == ''.join(dataset[i][1]):\n            count += 1\n            bleu_score[i] = sentence_bleu([dataset[i][1]], o1)\n            output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n            break;\n        elif j==0:\n            bleu_score[i] = sentence_bleu([dataset[i][1]], o1)\n            output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n\nf = open('accuracy_bw.txt','w')\nf.write(str(count/m))\nf.close()\n\nf = open('bleu_score_bw.txt','w')\nf.write(str(sum(bleu_score)/m))\nf.close()\n\nprint(count/m)\nprint(sum(bleu_score)/m)\n\nwith open('predicted_bw.csv','w') as file:\n    for line in output:\n        file.write(line)\n        file.write('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d51a10b8d091048fe0da1ad1889582a9047008b"},"cell_type":"code","source":"output = []\npad = '<pad>'\nm = len(dataset)\ns0 = np.zeros((m, n_s))\nbleu_score = np.zeros((m,1))\n#prediction = model.predict([Xoh, s0, c0])\nprediction = model.predict([Xoh, s0])\ncount = 0\nfor i in range(m):\n    p = np.argmax(np.array(prediction)[:,i,:], axis = 1)\n    p = int_to_string(p,inv_products_vocab)\n    o2 = []\n    for x in p:\n        if x != pad:\n            o2.append(x)\n    bleu_score[i] = sentence_bleu([dataset[i][1]], o2)\n    o2 = ''.join(o2)\n    if o2 == ''.join(dataset[i][1]):\n        count += 1\n    output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n\nprint(count/m)\nprint(sum(bleu_score)/m)\n\nf = open('accuracy.txt','w')\nf.write(str(count/m))\nf.close()\n\nf = open('bleu_score.txt','w')\nf.write(str(sum(bleu_score)/m))\nf.close()\n\nwith open('predicted.csv','w') as file:\n    for line in output:\n        file.write(line)\n        file.write('\\n')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}